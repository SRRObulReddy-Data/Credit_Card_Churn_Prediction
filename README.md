# Credit_Card_Churn_Prediction
## Abstract & Introduction

Opening fresh credit cards just to obtain the welcome bonus without planning to use them later is known as credit card churning. Although churning is not unlawful, card issuers are opposed to it and consider it unethical. Churning is not something credit card companies take too kindly. There are several steps that can be taken to make the procedure more challenging and less profitable for consumers. 

As credit cards become one of the most dependable and widely used methods of payment for both ordinary purchases and purchases made online. There is usually a practice of applying for multiple credit cards on a regular basis. Churners would open several credit cards in quick succession, get the welcome bonus for each new account, and then close or cease using the cards before credit card issuers really caught on and put procedures in place to halt the activity. Churners would restart with a new wave of applications a few months later. 

There were ways to do that even if they had to spend the bare minimum to qualify for the welcome incentives. This dataset gives a prediction on what percentage of people are doing the churning and who they are. This dataset helps the credit card issuers to know who exactly are performing churning. When a bank loses a customer due to any reason it is known as an attrited customer. 

This dataset helps to differentiate between the genuine attrited customer who has opted out of card and churning attrited customer who has opted out of card once they have taken all the benefits. The aim is to find the number of people who have opted out due to the process of churning and predict them using PySpark and modelling.

## Implementation 

Data Conversion: The conversion of data from CSV to Parquet has been done, made use of Apache Parquet. PySpark is utilized, and Machine learning modelling has been done in Jupyter Notebook. Additionally, visualizations using the pandas, matplotlib, and sns libraries were done in Jupyter Notebook.

Data Loading: Data is extracted into the virtual machine (Microsoft Azure), Jupyter notebook is used. The session is started when the spark is started running simultaneously.


Let's Goooo...!!!
